from "tokenizer" import
    make_tokenizer,
    Token,
    TokenizeError,
    Tokenizer,
    TokenizeResult,


fn print_tokens args tokens as vec[Token] {
    tokens
    foreach {
        use token {
            token .
            "\n" .
        }
    }

    "\n" .
}

fn main args argv as vec[str] return int {
    if argv vec:len 2 != {
        "Expected 1 argument.\n" .
        "Usage: " .
        argv 0 vec:get .
        " <json string>\n" .
        1 return
    }

    argv 1 vec:get
    use input {
        input make_tokenizer Tokenizer:run
    }

    match {
        case TokenizeResult:error as tokens, error {
            "Tokenizing failed:\n" .
            tokens print_tokens
            error TokenizeError:to_str .
            "\n" .
            1 return
        }
        case TokenizeResult:ok as tokens { tokens }
    }

    "Tokenizing successful:\n" .

    use tokens {
        tokens print_tokens
    }

    "Parser is not implemented yet\n" .
    0 return
}
