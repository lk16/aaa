struct Range {
    next_value as int,
    end as int,
}

fn make_range args start as int, end as int return Range {
    Range
    dup "next_value" { start } !
    dup "end" { end } !
}

fn Range:iter args r as Range return Range {
    r
}

fn Range:next args r as Range return int, bool {
    r "next_value" ?
    r "end" ?
    use next_value, end {
        if next_value end >= {
            0 false return
        }

        r "next_value" { next_value 1 + } !
        next_value true
    }
}

enum FileReadResult {
    ok as str,
    open_error,
    read_error,
}

fn read_file args path as const str, return FileReadResult {
    path 0 0 open

    use fd, open_ok {
        if open_ok not {
            FileReadResult:open_error return
        }

        ""
        use content {
            while true {
                fd 4096 read

                use buff, read_ok {
                    if read_ok not {
                        FileReadResult:read_error return
                    }

                    if buff "" = {
                        content FileReadResult:ok return
                    }

                    content <- { content buff str:append }
                }
            }
        }
    }
}

enum TokenType {
    args_,
    as_,
    assign,
    builtin_,
    call_,
    case_,
    char_,
    colon,
    comma,
    comment,
    const_,
    default_,
    else_,
    end,
    enum_,
    false_,
    fn_,
    foreach_,
    from_,
    get_field,
    identifier,
    if_,
    import_,
    integer,
    match_,
    never_,
    return_,
    set_field,
    sq_end,
    sq_start,
    start,
    string,
    struct_,
    true_,
    use_,
    while_,
    whitespace,
}

fn TokenType:to_str args token_type as const TokenType return str {
    token_type
    match {
        case TokenType:args_ { "args" }
        case TokenType:as_ { "as" }
        case TokenType:assign { "assign" }
        case TokenType:builtin_ { "builtin" }
        case TokenType:call_ { "call" }
        case TokenType:case_ { "case" }
        case TokenType:char_ { "char" }
        case TokenType:colon { "colon" }
        case TokenType:comma { "comma" }
        case TokenType:comment { "comment" }
        case TokenType:const_ { "const" }
        case TokenType:default_ { "default" }
        case TokenType:else_ { "else" }
        case TokenType:end { "end" }
        case TokenType:enum_ { "enum" }
        case TokenType:false_ { "false" }
        case TokenType:fn_ { "fn" }
        case TokenType:foreach_ { "foreach" }
        case TokenType:from_ { "from" }
        case TokenType:get_field { "get_field" }
        case TokenType:identifier { "identifier" }
        case TokenType:if_ { "if" }
        case TokenType:import_ { "import" }
        case TokenType:integer { "integer" }
        case TokenType:match_ { "match" }
        case TokenType:never_ { "never" }
        case TokenType:return_ { "return" }
        case TokenType:set_field { "set_field" }
        case TokenType:sq_end { "sq_end" }
        case TokenType:sq_start { "sq_start" }
        case TokenType:start { "start" }
        case TokenType:string { "string" }
        case TokenType:struct_ { "struct" }
        case TokenType:true_ { "true" }
        case TokenType:use_ { "use" }
        case TokenType:while_ { "while" }
        case TokenType:whitespace { "whitespace" }
    }
}

struct FixedSizedToken {
    value as str,
    type_ as TokenType,
}

fn make_fixed_sized_token args value as str, type_ as TokenType return FixedSizedToken {
    FixedSizedToken
    dup "type_" { type_ } !
    dup "value" { value } !
}

fn get_fixed_sized_tokens return vec[FixedSizedToken] {
    vec[FixedSizedToken]
    use v {
        // NOTE: keep this sorted by longest first, and for same length alphabetical
        v "builtin" TokenType:builtin_             make_fixed_sized_token vec:push
        v "default" TokenType:default_             make_fixed_sized_token vec:push
        v "foreach" TokenType:foreach_             make_fixed_sized_token vec:push
        v "import"  TokenType:import_              make_fixed_sized_token vec:push
        v "return"  TokenType:return_              make_fixed_sized_token vec:push
        v "struct"  TokenType:struct_              make_fixed_sized_token vec:push
        v "const"   TokenType:const_               make_fixed_sized_token vec:push
        v "false"   TokenType:false_               make_fixed_sized_token vec:push
        v "match"   TokenType:match_               make_fixed_sized_token vec:push
        v "never"   TokenType:never_               make_fixed_sized_token vec:push
        v "while"   TokenType:while_               make_fixed_sized_token vec:push
        v "args"    TokenType:args_                make_fixed_sized_token vec:push
        v "call"    TokenType:call_                make_fixed_sized_token vec:push
        v "case"    TokenType:case_                make_fixed_sized_token vec:push
        v "else"    TokenType:else_                make_fixed_sized_token vec:push
        v "enum"    TokenType:enum_                make_fixed_sized_token vec:push
        v "from"    TokenType:from_                make_fixed_sized_token vec:push
        v "true"    TokenType:true_                make_fixed_sized_token vec:push
        v "use"     TokenType:use_                 make_fixed_sized_token vec:push
        v "!="      TokenType:identifier           make_fixed_sized_token vec:push
        v "<-"      TokenType:assign               make_fixed_sized_token vec:push
        v "<="      TokenType:identifier           make_fixed_sized_token vec:push
        v ">="      TokenType:identifier           make_fixed_sized_token vec:push
        v "as"      TokenType:as_                  make_fixed_sized_token vec:push
        v "fn"      TokenType:fn_                  make_fixed_sized_token vec:push
        v "if"      TokenType:if_                  make_fixed_sized_token vec:push
        v "-"       TokenType:identifier           make_fixed_sized_token vec:push
        v ","       TokenType:comma                make_fixed_sized_token vec:push
        v ":"       TokenType:colon                make_fixed_sized_token vec:push
        v "!"       TokenType:set_field            make_fixed_sized_token vec:push
        v "?"       TokenType:get_field            make_fixed_sized_token vec:push
        v "."       TokenType:identifier           make_fixed_sized_token vec:push
        v "["       TokenType:sq_start             make_fixed_sized_token vec:push
        v "]"       TokenType:sq_end               make_fixed_sized_token vec:push
        v "{"       TokenType:start                make_fixed_sized_token vec:push
        v "}"       TokenType:end                  make_fixed_sized_token vec:push
        v "*"       TokenType:identifier           make_fixed_sized_token vec:push
        v "/"       TokenType:identifier           make_fixed_sized_token vec:push
        v "%"       TokenType:identifier           make_fixed_sized_token vec:push
        v "+"       TokenType:identifier           make_fixed_sized_token vec:push
        v "<"       TokenType:identifier           make_fixed_sized_token vec:push
        v "="       TokenType:identifier           make_fixed_sized_token vec:push
        v ">"       TokenType:identifier           make_fixed_sized_token vec:push

        v
    }
}

struct Position {
    file as str,
    line as int,
    column as int,
}

fn make_position args file as str, line as int, column as int return Position {
    Position
    dup "file" { file } !
    dup "line" { line } !
    dup "column" { column } !
}

fn Position:update args position as Position, token as Token {
    token "value" ?
    use value {
        0 value str:len make_range
        foreach {
            use i {

                value i str:at
                use char_, ok {
                    if ok not {
                        unreachable
                    }

                    if char_ '\n' = {
                        position "line" { position "line" ? 1 + } !
                        position "column" { 1 } !
                    }
                    else {
                        position "column" { position "column" ? 1 + } !
                    }
                }
            }
        }
    }
}

struct Token {
    position as Position,
    type_ as TokenType,
    value as str,
}

fn Token:= args lhs as const Token, rhs as const Token return bool {
    if
        lhs "type_" ? TokenType:to_str
        rhs "type_" ? TokenType:to_str
        = not
    {
        false return
    }

    lhs "value" ?
    rhs "value" ?
    =
}

enum OptionalToken {
    some as Token,
    none,
}

struct Tokenizer {
    filename as str,
    input as str,
    offset as int,
    offset_position as Position,
    fixed_sized_tokens as vec[FixedSizedToken],
    whitespace_regex as regex,
    comment_regex as regex,
    integer_regex as regex,
    identifier_regex as regex,
    string_regex as regex,
    character_regex as regex,
}

fn make_tokenizer args filename as str, input as str return Tokenizer {
    Tokenizer
    dup "filename" { filename } !
    dup "input" { input } !
    dup "offset_position" { filename 1 1 make_position } !
    dup "fixed_sized_tokens" { get_fixed_sized_tokens } !
    dup "whitespace_regex" { "\\s+" make_regex assert } !
    dup "comment_regex" { "//[^\n]*" make_regex assert } !
    dup "integer_regex" { "(-)?[0-9]+" make_regex assert } !
    dup "identifier_regex" { "[a-zA-Z_]+" make_regex assert } !
    dup "string_regex" { "\"(\\\\.|.|\n)*?\"" make_regex assert } !
    dup "character_regex" { "'(\\\\.|.|\n)'" make_regex assert } !
}

fn Tokenizer:tokenize_whitespace args tokenizer as Tokenizer return OptionalToken {
    tokenizer "input" ?
    tokenizer "offset" ?
    tokenizer "whitespace_regex" ?
    use input, offset, whitespace_regex {
        whitespace_regex input offset regex:find
        use matched_str, matched_offset, matched {
            if matched not matched_offset offset != or {
                OptionalToken:none
            } else {
                Token
                dup "type_" { TokenType:whitespace } !
                dup "value" { matched_str } !
                OptionalToken:some
            }
        }
    }
}

fn Tokenizer:tokenize_comment args tokenizer as Tokenizer return OptionalToken {
    tokenizer "input" ?
    tokenizer "offset" ?
    tokenizer "comment_regex" ?
    use input, offset, comment_regex {
        comment_regex input offset regex:find
        use matched_str, matched_offset, matched {
            if matched not matched_offset offset != or {
                OptionalToken:none return
            }

            Token
            dup "type_" { TokenType:comment } !
            dup "value" { matched_str } !
            OptionalToken:some
        }
    }
}

fn Tokenizer:tokenize_integer args tokenizer as Tokenizer return OptionalToken {
    tokenizer "input" ?
    tokenizer "offset" ?
    tokenizer "integer_regex" ?
    use input, offset, integer_regex {
        integer_regex input offset regex:find
        use matched_str, matched_offset, matched {
            if matched not matched_offset offset != or {
                OptionalToken:none return
            }

            Token
            dup "type_" { TokenType:integer } !
            dup "value" { matched_str } !
            OptionalToken:some
        }
    }
}

fn matches_at args input as const str, search as str, offset as int return bool {
    input search offset str:find_after
    use found_offset, ok {
        ok offset found_offset = and
    }
}

fn Tokenizer:match_fixed_size args tokenizer as Tokenizer, fixed_sized_token as FixedSizedToken return OptionalToken {
    tokenizer "input" ?
    tokenizer "offset" ?
    fixed_sized_token "value" ?
    fixed_sized_token "type_" ?
    use input, offset, token_value, token_type {
        if input token_value offset matches_at not {
            OptionalToken:none return
        }

        // prevent matching identifier `asdf` as keyword `as`
        if
            input offset token_value str:len + is_identifier_char
            token_value 0 is_identifier_char
            and
        {
            OptionalToken:none return
        }

        Token
        dup "type_" { token_type } !
        dup "value" { token_value } !
        OptionalToken:some
    }
}

fn Tokenizer:tokenize_fixed_size args tokenizer as Tokenizer return OptionalToken {
    tokenizer "fixed_sized_tokens" ?
    foreach {
        use fixed_sized_token {
            tokenizer fixed_sized_token Tokenizer:match_fixed_size
            match {
                case OptionalToken:some {
                    use optional_token {
                        drop
                        optional_token OptionalToken:some return
                    }
                }
                default {
                    nop
                }
            }
        }
    }

    OptionalToken:none
}

fn Tokenizer:tokenize_string args tokenizer as Tokenizer return OptionalToken {
    tokenizer "input" ?
    tokenizer "offset" ?
    tokenizer "string_regex" ?
    use input, offset, string_regex {
        string_regex input offset regex:find
        use matched_str, matched_offset, matched {
            if matched matched_offset offset = and not {
                OptionalToken:none return
            }

            Token
            dup "type_" { TokenType:string } !
            dup "value" { matched_str } !
            OptionalToken:some
        }
    }
}

fn Tokenizer:tokenize_character args tokenizer as Tokenizer return OptionalToken {
    tokenizer "input" ?
    tokenizer "offset" ?
    tokenizer "character_regex" ?
    use input, offset, character_regex {
        character_regex input offset regex:find
        use matched_str, matched_offset, matched {
            if matched matched_offset offset = and not {
                OptionalToken:none return
            }

            Token
            dup "type_" { TokenType:char_ } !
            dup "value" { matched_str } !
            OptionalToken:some

        }
    }
}

fn is_identifier_char args input as const str, offset as int return bool {
    input offset str:at
    use char_, ok {
        if ok not {
            false return
        }

        char_ char:is_alpha
        char_ '_' =
        or
    }
}

fn Tokenizer:tokenize_identifier args tokenizer as Tokenizer return OptionalToken {
    tokenizer "input" ?
    tokenizer "offset" ?
    tokenizer "identifier_regex" ?
    use input, offset, identifier_regex {
        identifier_regex input offset regex:find
        use matched_str, matched_offset, matched {
            if matched not matched_offset offset != or {
                OptionalToken:none return
            }

            Token
            dup "type_" { TokenType:identifier } !
            dup "value" { matched_str } !
            OptionalToken:some
        }
    }
}

fn Tokenizer:tokenize_at_offset args tokenizer as Tokenizer return OptionalToken {
    tokenizer Tokenizer:tokenize_whitespace
    match {
        case OptionalToken:some { OptionalToken:some return }
        default { nop }
    }

    tokenizer Tokenizer:tokenize_comment
    match {
        case OptionalToken:some { OptionalToken:some return }
        default { nop }
    }

    tokenizer Tokenizer:tokenize_integer
    match {
        case OptionalToken:some { OptionalToken:some return }
        default { nop }
    }

    tokenizer Tokenizer:tokenize_fixed_size
    match {
        case OptionalToken:some { OptionalToken:some return }
        default { nop }
    }

    tokenizer Tokenizer:tokenize_string
    match {
        case OptionalToken:some { OptionalToken:some return }
        default { nop }
    }

    tokenizer Tokenizer:tokenize_identifier
    match {
        case OptionalToken:some { OptionalToken:some return }
        default { nop }
    }

    tokenizer Tokenizer:tokenize_character
}

enum TokenizeResult {
    ok as vec[Token],
    error,
}

fn Tokenizer:run args tokenizer as Tokenizer return TokenizeResult {
    vec[Token]
    tokenizer "input" ?
    use tokens, input {
        while tokenizer "offset" ? input str:len < {
            tokenizer Tokenizer:tokenize_at_offset
            match {
                case OptionalToken:none {
                    TokenizeResult:error return
                }
                case OptionalToken:some {
                    use token {
                        token "position" { tokenizer "offset_position" ? copy swap drop } !

                        tokens token vec:push
                        tokenizer "offset" {
                            tokenizer "offset" ?
                            token "value" ? str:len
                            +
                        } !
                        tokenizer "offset_position" ? token Position:update
                    }
                }
            }
        }
        tokens TokenizeResult:ok return
    }
}

fn print_tokens args tokens as vec[Token] {
    tokens
    foreach {
        dup drop
        use token {
            token "type_" ?
            match {
                case TokenType:whitespace { nop }
                case TokenType:comment { nop }
                default {
                    token "position" ?
                    use position {
                        position "file" ? .
                        ":" .
                        position "line" ? .
                        ":" .
                        position "column" ? .
                    }
                    " " .
                    token "type_" ? TokenType:to_str .
                    " " .
                    token "value" ? .
                    "\n" .
                }
            }
        }
    }
}

fn main args argv as vec[str] return int {
    if argv vec:len 2 != {
        "Usage: " .
        argv 0 vec:get .
        " <source_file>\n" .
        1 return
    }

    argv 1 vec:get

    use source_path {
        source_path read_file
        match {
            case FileReadResult:open_error {
                "Could not open " . source_path . "\n" .
                1 return
            }
            case FileReadResult:read_error {
                "Could not read " . source_path . "\n" .
                1 return
            }
            case FileReadResult:ok {
                use content {
                    source_path content make_tokenizer Tokenizer:run
                    match {
                        case TokenizeResult:ok {
                            use tokens {
                                tokens print_tokens
                            }
                        }
                        case TokenizeResult:error {
                            "Tokenization failed.\n" .
                            1 return
                        }
                    }
                }
            }
        }
    }

    0
}
