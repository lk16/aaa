from "../../stdlib/enums.aaa" import Option

from "tokenizer" import
    make_tokenizer,
    Token,
    Tokenizer,
    TokenType,


fn check_tokenize_whitespace_fail args input as str, offset as int {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_whitespace
    match {
        case Option:some { unreachable }
        case Option:none { nop }
    }
}

fn test_tokenize_whitespace_fail {
    "" 0 check_tokenize_whitespace_fail
    "." 0 check_tokenize_whitespace_fail
    "a" 0 check_tokenize_whitespace_fail
    "\\" 0 check_tokenize_whitespace_fail
    "/" 0 check_tokenize_whitespace_fail
}

fn check_tokenize_whitespace args input as str, offset as int, expected_value as const str {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_whitespace
    match {
        case Option:some {
            use token_ {
                token_ "value" ?
                use value {
                    value expected_value = assert
                }
            }
        }
        case Option:none { unreachable }
    }
}

fn test_tokenize_whitespace_ok {
    "a a" 1 " " check_tokenize_whitespace
    "a\na" 1 "\n" check_tokenize_whitespace
    "a\ra" 1 "\r" check_tokenize_whitespace
    "a \n  \r\r\n \n\na" 1 " \n  \r\r\n \n\n" check_tokenize_whitespace
}

fn check_tokenize_comment_fail args input as str, offset as int {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_comment
    match {
        case Option:some { unreachable }
        case Option:none { nop }
    }
}

fn test_tokenize_comment_fail {
    "" 0 check_tokenize_comment_fail
    "." 0 check_tokenize_comment_fail
    "a" 0 check_tokenize_comment_fail
    "/" 0 check_tokenize_comment_fail
    "a/" 0 check_tokenize_comment_fail
    "/ab" 0 check_tokenize_comment_fail
}

fn check_tokenize_comment args input as str, offset as int, expected_value as const str {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_comment
    match {
        case Option:some {
            use token_ {
                token_ "value" ?
                use value {
                    value expected_value = assert
                }
            }
        }
        case Option:none { unreachable }
    }
}

fn test_tokenize_comment_ok {
    "//" 0 "//" check_tokenize_comment
    "// something " 0 "// something " check_tokenize_comment
    "a// something " 1 "// something " check_tokenize_comment
    "// something \nb" 0 "// something " check_tokenize_comment
    "a// something \nb" 1 "// something " check_tokenize_comment
    "a// something \n" 1 "// something " check_tokenize_comment
}

fn check_tokenize_integer_fail args input as str, offset as int {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_integer
    match {
        case Option:some { unreachable }
        case Option:none { nop }
    }
}

fn test_tokenize_integer_fail {
    "" 0 check_tokenize_integer_fail
    "." 0 check_tokenize_integer_fail
    "a" 0 check_tokenize_integer_fail
    "/" 0 check_tokenize_integer_fail
    "\\" 0 check_tokenize_integer_fail
    "-" 0 check_tokenize_integer_fail
}

fn check_tokenize_integer args input as str, offset as int, expected_value as const str {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_integer
    match {
        case Option:some {
            use token_ {
                token_ "value" ?
                use value {
                    value expected_value = assert
                }
            }
        }
        case Option:none { unreachable }
    }
    "123a" 0 "123" check_tokenize_integer
    "a123a" 1 "123" check_tokenize_integer
    "a123" 1 "123" check_tokenize_integer
    "-123a" 0 "-123" check_tokenize_integer
    "a-123a" 1 "-123" check_tokenize_integer
    "a-123" 1 "-123" check_tokenize_integer
}

fn check_tokenize_fixed_size_fail args input as str, offset as int {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_fixed_size
    match {
        case Option:some { unreachable }
        case Option:none { nop }
    }
}

fn test_tokenize_fixed_size_fail {
    "argsabc" 0 check_tokenize_fixed_size_fail
    "asabc" 0 check_tokenize_fixed_size_fail
    "caseabc" 0 check_tokenize_fixed_size_fail
    "constabc" 0 check_tokenize_fixed_size_fail
    "defaultabc" 0 check_tokenize_fixed_size_fail
    "elseabc" 0 check_tokenize_fixed_size_fail
    "enumabc" 0 check_tokenize_fixed_size_fail
    "falseabc" 0 check_tokenize_fixed_size_fail
    "fnabc" 0 check_tokenize_fixed_size_fail
    "foreachabc" 0 check_tokenize_fixed_size_fail
    "fromabc" 0 check_tokenize_fixed_size_fail
    "ifabc" 0 check_tokenize_fixed_size_fail
    "importabc" 0 check_tokenize_fixed_size_fail
    "matchabc" 0 check_tokenize_fixed_size_fail
    "neverabc" 0 check_tokenize_fixed_size_fail
    "returnabc" 0 check_tokenize_fixed_size_fail
    "structabc" 0 check_tokenize_fixed_size_fail
    "trueabc" 0 check_tokenize_fixed_size_fail
    "typeabc" 0 check_tokenize_fixed_size_fail
    "useabc" 0 check_tokenize_fixed_size_fail
    "whileabc" 0 check_tokenize_fixed_size_fail
}

fn check_tokenize_fixed_size args input as str, offset as int, expected_type as TokenType, expected_value as str {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_fixed_size
    match {
        case Option:some {
            use token_ {
                token_

                Token
                dup "type_" { expected_type } !
                dup "value" { expected_value } !

                = assert
            }
        }
        case Option:none { unreachable }
    }
}

fn test_tokenize_fixed_size_ok_bare {
    "-" 0 TokenType:identifier "-" check_tokenize_fixed_size
    "," 0 TokenType:comma "," check_tokenize_fixed_size
    ":" 0 TokenType:colon ":" check_tokenize_fixed_size
    "!" 0 TokenType:set_field "!" check_tokenize_fixed_size
    "?" 0 TokenType:get_field "?" check_tokenize_fixed_size
    "." 0 TokenType:identifier "." check_tokenize_fixed_size
    "[" 0 TokenType:sq_start "[" check_tokenize_fixed_size
    "]" 0 TokenType:sq_end "]" check_tokenize_fixed_size
    "{" 0 TokenType:start "{" check_tokenize_fixed_size
    "}" 0 TokenType:end "}" check_tokenize_fixed_size
    "*" 0 TokenType:identifier "*" check_tokenize_fixed_size
    "/" 0 TokenType:identifier "/" check_tokenize_fixed_size
    "%" 0 TokenType:identifier "%" check_tokenize_fixed_size
    "+" 0 TokenType:identifier "+" check_tokenize_fixed_size
    "<-" 0 TokenType:assign "<-" check_tokenize_fixed_size
    "<" 0 TokenType:identifier "<" check_tokenize_fixed_size
    "<=" 0 TokenType:identifier "<=" check_tokenize_fixed_size
    "=" 0 TokenType:identifier "=" check_tokenize_fixed_size
    ">" 0 TokenType:identifier ">" check_tokenize_fixed_size
    ">=" 0 TokenType:identifier ">=" check_tokenize_fixed_size
    "args" 0 TokenType:args_ "args" check_tokenize_fixed_size
    "as" 0 TokenType:as_ "as" check_tokenize_fixed_size
    "call" 0 TokenType:call_ "call" check_tokenize_fixed_size
    "case" 0 TokenType:case_ "case" check_tokenize_fixed_size
    "const" 0 TokenType:const_ "const" check_tokenize_fixed_size
    "default" 0 TokenType:default_ "default" check_tokenize_fixed_size
    "else" 0 TokenType:else_ "else" check_tokenize_fixed_size
    "enum" 0 TokenType:enum_ "enum" check_tokenize_fixed_size
    "false" 0 TokenType:false_ "false" check_tokenize_fixed_size
    "fn" 0 TokenType:fn_ "fn" check_tokenize_fixed_size
    "foreach" 0 TokenType:foreach_ "foreach" check_tokenize_fixed_size
    "from" 0 TokenType:from_ "from" check_tokenize_fixed_size
    "if" 0 TokenType:if_ "if" check_tokenize_fixed_size
    "import" 0 TokenType:import_ "import" check_tokenize_fixed_size
    "match" 0 TokenType:match_ "match" check_tokenize_fixed_size
    "never" 0 TokenType:never_ "never" check_tokenize_fixed_size
    "return" 0 TokenType:return_ "return" check_tokenize_fixed_size
    "struct" 0 TokenType:struct_ "struct" check_tokenize_fixed_size
    "true" 0 TokenType:true_ "true" check_tokenize_fixed_size
    "use" 0 TokenType:use_ "use" check_tokenize_fixed_size
    "while" 0 TokenType:while_ "while" check_tokenize_fixed_size
}

fn test_tokenize_fixed_size_ok_middle {
    " - " 1 TokenType:identifier "-" check_tokenize_fixed_size
    " , " 1 TokenType:comma "," check_tokenize_fixed_size
    " : " 1 TokenType:colon ":" check_tokenize_fixed_size
    " ! " 1 TokenType:set_field "!" check_tokenize_fixed_size
    " ? " 1 TokenType:get_field "?" check_tokenize_fixed_size
    " . " 1 TokenType:identifier "." check_tokenize_fixed_size
    " [ " 1 TokenType:sq_start "[" check_tokenize_fixed_size
    " ] " 1 TokenType:sq_end "]" check_tokenize_fixed_size
    " { " 1 TokenType:start "{" check_tokenize_fixed_size
    " } " 1 TokenType:end "}" check_tokenize_fixed_size
    " * " 1 TokenType:identifier "*" check_tokenize_fixed_size
    " / " 1 TokenType:identifier "/" check_tokenize_fixed_size
    " % " 1 TokenType:identifier "%" check_tokenize_fixed_size
    " + " 1 TokenType:identifier "+" check_tokenize_fixed_size
    " <- " 1 TokenType:assign "<-" check_tokenize_fixed_size
    " < " 1 TokenType:identifier "<" check_tokenize_fixed_size
    " <= " 1 TokenType:identifier "<=" check_tokenize_fixed_size
    " = " 1 TokenType:identifier "=" check_tokenize_fixed_size
    " > " 1 TokenType:identifier ">" check_tokenize_fixed_size
    " >= " 1 TokenType:identifier ">=" check_tokenize_fixed_size
    " args " 1 TokenType:args_ "args" check_tokenize_fixed_size
    " as " 1 TokenType:as_ "as" check_tokenize_fixed_size
    " call " 1 TokenType:call_ "call" check_tokenize_fixed_size
    " case " 1 TokenType:case_ "case" check_tokenize_fixed_size
    " const " 1 TokenType:const_ "const" check_tokenize_fixed_size
    " default " 1 TokenType:default_ "default" check_tokenize_fixed_size
    " else " 1 TokenType:else_ "else" check_tokenize_fixed_size
    " enum " 1 TokenType:enum_ "enum" check_tokenize_fixed_size
    " false " 1 TokenType:false_ "false" check_tokenize_fixed_size
    " fn " 1 TokenType:fn_ "fn" check_tokenize_fixed_size
    " foreach " 1 TokenType:foreach_ "foreach" check_tokenize_fixed_size
    " from " 1 TokenType:from_ "from" check_tokenize_fixed_size
    " if " 1 TokenType:if_ "if" check_tokenize_fixed_size
    " import " 1 TokenType:import_ "import" check_tokenize_fixed_size
    " match " 1 TokenType:match_ "match" check_tokenize_fixed_size
    " never " 1 TokenType:never_ "never" check_tokenize_fixed_size
    " return " 1 TokenType:return_ "return" check_tokenize_fixed_size
    " struct " 1 TokenType:struct_ "struct" check_tokenize_fixed_size
    " true " 1 TokenType:true_ "true" check_tokenize_fixed_size
    " use " 1 TokenType:use_ "use" check_tokenize_fixed_size
    " while " 1 TokenType:while_ "while" check_tokenize_fixed_size
}

fn check_tokenize_string_fail args input as str, offset as int {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_string
    match {
        case Option:some { unreachable }
        case Option:none { nop }
    }
}

fn test_tokenize_string_fail {
    "" 0 check_tokenize_string_fail
    "a" 0 check_tokenize_string_fail
    "." 0 check_tokenize_string_fail
    "." 0 check_tokenize_string_fail
    "\"" 0 check_tokenize_string_fail
    "a\"" 0 check_tokenize_string_fail
    "\"a" 0 check_tokenize_string_fail
    "\"\\" 0 check_tokenize_string_fail
}

fn check_tokenize_string args input as str, offset as int, expected_value as const str {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_string
    match {
        case Option:some {
            use token_ {
                token_ "value" ?
                use value {
                    value expected_value = assert
                }
            }
        }
        case Option:none { unreachable }
    }
}

fn test_tokenize_string_ok {
    "\"\"" 0 "\"\"" check_tokenize_string
    "\"\" " 0 "\"\"" check_tokenize_string
    " \"\"" 1 "\"\"" check_tokenize_string
    " \"\" " 1 "\"\"" check_tokenize_string
    "\"a\"" 0 "\"a\"" check_tokenize_string
    " \"a\"" 1 "\"a\"" check_tokenize_string
    " \"a\" " 1 "\"a\"" check_tokenize_string
    "\"\n\"" 0 "\"\n\"" check_tokenize_string
    " \"\n\"" 1 "\"\n\"" check_tokenize_string
    " \"\n\" " 1 "\"\n\"" check_tokenize_string
    "\"\\n\"" 0 "\"\\n\"" check_tokenize_string
    " \"\\n\"" 1 "\"\\n\"" check_tokenize_string
    " \"\\n\" " 1 "\"\\n\"" check_tokenize_string
    "\"\\\\\"" 0 "\"\\\\\"" check_tokenize_string
    " \"\\\\\"" 1 "\"\\\\\"" check_tokenize_string
    " \"\\\\\" " 1 "\"\\\\\"" check_tokenize_string
}

fn check_tokenize_identifier_fail args input as str, offset as int {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_string
    match {
        case Option:some { unreachable }
        case Option:none { nop }
    }
}

fn test_tokenize_identifier_fail {
    "" 0 check_tokenize_identifier_fail
    "3" 0 check_tokenize_identifier_fail
    "." 0 check_tokenize_identifier_fail
    "\n" 0 check_tokenize_identifier_fail
}

fn check_tokenize_identifier args input as str, offset as int, expected_value as const str {
    "" input make_tokenizer
    dup "offset" { offset } !
    Tokenizer:tokenize_identifier
    match {
        case Option:some {
            use token_ {
                token_ "value" ?
                use value {
                    value expected_value = assert
                }
            }
        }
        case Option:none { unreachable }
    }
}

fn test_tokenize_identifier_ok {
    "a" 0 "a" check_tokenize_identifier
    "z" 0 "z" check_tokenize_identifier
    "A" 0 "A" check_tokenize_identifier
    "Z" 0 "Z" check_tokenize_identifier
    "_" 0 "_" check_tokenize_identifier

    " a " 1 "a" check_tokenize_identifier
    " z " 1 "z" check_tokenize_identifier
    " A " 1 "A" check_tokenize_identifier
    " Z " 1 "Z" check_tokenize_identifier
    " _ " 1 "_" check_tokenize_identifier

    "aaaa" 0 "aaaa" check_tokenize_identifier
    "zzzz" 0 "zzzz" check_tokenize_identifier
    "AAAA" 0 "AAAA" check_tokenize_identifier
    "ZZZZ" 0 "ZZZZ" check_tokenize_identifier
    "____" 0 "____" check_tokenize_identifier
}
